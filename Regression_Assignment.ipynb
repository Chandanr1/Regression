{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "   - Models the relationship between one independent variable (X) and one dependent variable (Y) using a straight line.\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "    - Key Assumptions:\n",
        "        Linearity\n",
        "        Independence\n",
        "        Homoscedasticity\n",
        "        Normality of residuals\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "    - Coefficient m: Represents the slope; the change in Y for a unit change in X.\n",
        "\n",
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        "    -  Intercept c: The predicted value of Y when X = 0.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "    - (y2-y1)/(x2-x1)\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "    - Purpose of Least Squares: Minimizes the sum of squared differences between actual and predicted Y values.\n",
        "\n",
        "7.  How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression?\n",
        "R¬≤ Interpretation: Proportion of variance in Y explained by X (ranges from 0 to 1).\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "    - Definition: Extends simple regression to multiple independent variables.\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "    - Main Difference:\n",
        "      Simple: 1 independent variable\n",
        "      Multiple: 2 or more independent variables\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "    - Key Assumptions:\n",
        "      Linearity\n",
        "      Independence\n",
        "      Homoscedasticity\n",
        "      No multicollinearity\n",
        "      Normal distribution of errors\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "Heteroscedasticity: Non-constant variance of errors; leads to inefficient estimates.\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "     - Handling Multicollinearity:\n",
        "      Remove/reduce correlated predictors\n",
        "      Use PCA or regularization (Ridge, Lasso)\n",
        "13. - What are some common techniques for transforming categorical variables for use in regression models?\n",
        "     - Transforming Categorical Variables:\n",
        "      One-hot encoding\n",
        "      Label encoding\n",
        "      Dummy variables\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "Interaction Terms: Capture combined effects of two or more variables on Y.\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "      - Intercept Interpretation:\n",
        "        Simple: Y when X=0\n",
        "        Multiple: Y when all Xs = 0 (may be unrealistic)\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "      - Slope Significance: Indicates strength and direction of the relationship; affects predictions directly.\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "      - Intercept Context: Sets the baseline value of Y; useful for understanding starting point of prediction.\n",
        "\n",
        "18. What are the limitations of using R¬≤ as a sole measure of model performance?\n",
        "      - R¬≤ Limitations:\n",
        "\n",
        "Doesn‚Äôt indicate causality\n",
        "\n",
        "Can increase with irrelevant variables\n",
        "\n",
        "Doesn‚Äôt account for model complexity\n",
        "\n",
        "19.  How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "      - Large Standard Error: Indicates uncertainty in coefficient estimate; may be statistically insignificant.\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "    - Identifying Heteroscedasticity: Residual plots show funnel shape; affects validity of tests.\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R¬≤ but low adjusted R¬≤?\n",
        "    - High R¬≤, Low Adjusted R¬≤: Indicates overfitting or inclusion of irrelevant variables.\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "    - Importance of Scaling: Prevents features with large ranges from dominating the model; needed for regularization.\n",
        "\n",
        "23.  What is polynomial regression?\n",
        "    - Definition: Regression where the relationship between X and Y is modeled as an nth-degree polynomial.\n",
        "\n",
        "24. How does polynomial regression differ from linear regression?\n",
        "    - Difference from Linear: Captures non-linear patterns; linear regression fits a straight line.\n",
        "\n",
        "25. When is polynomial regression used?\n",
        "    - When Used: When data shows a non-linear relationship.\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "    - General Equation:ùëå=ùëè0+ùëè1ùëã+ùëè2ùëã2+‚ãØ+ùëèùëõùëãùëõ\n",
        "\n",
        "27.  Can polynomial regression be applied to multiple variables?\n",
        "    - Multiple Variables: Yes, polynomial terms can be added for multiple variables (e.g., X1^2 ,X1X2)\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        "    - Limitations:\n",
        "      Overfitting\n",
        "      Extrapolation issues\n",
        "      Interpretation becomes complex\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "      - Evaluating Degree:\n",
        "        Cross-validation\n",
        "        Adjusted R¬≤\n",
        "        AIC/BIC\n",
        "        Visualization\n",
        "\n",
        "30. Why is visualization important in polynomial regression?\n",
        "      - Importance of Visualization: Helps identify underfitting/overfitting and interpret the curve fit."
      ],
      "metadata": {
        "id": "WC6UQmbSlDGz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fQYNtl3KlCKP"
      },
      "outputs": [],
      "source": [
        "#Ans 31\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "model.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PN8H8DLDqYhU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}